---
annotation-target: ../zhai2016.pdf
---


>%%
>```annotation-json
>{"created":"2024-09-09T13:33:15.332Z","updated":"2024-09-09T13:33:15.332Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":617558,"end":617562},{"type":"TextQuoteSelector","exact":"14.1","prefix":" end with clustering evaluation.","suffix":" Overview of Clustering Techniqu"}]}]}
>```
>%%
>*%%PREFIX%%end with clustering evaluation.%%HIGHLIGHT%% ==14.1== %%POSTFIX%%Overview of Clustering Techniqu*
>%%LINK%%[[#^iw4d216l63|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^iw4d216l63


>%%
>```annotation-json
>{"created":"2024-09-09T13:33:21.119Z","updated":"2024-09-09T13:33:21.119Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":622657,"end":622661},{"type":"TextQuoteSelector","exact":"14.2","prefix":"nized and term IDs are assigned.","suffix":" Document ClusteringIn this sect"}]}]}
>```
>%%
>*%%PREFIX%%nized and term IDs are assigned.%%HIGHLIGHT%% ==14.2== %%POSTFIX%%Document ClusteringIn this sect*
>%%LINK%%[[#^xpghc7ug49s|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^xpghc7ug49s


>%%
>```annotation-json
>{"created":"2024-09-09T13:33:51.525Z","updated":"2024-09-09T13:33:51.525Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":648050,"end":648058},{"type":"TextQuoteSelector","exact":"14.3.3.2","prefix":"(see, e.g., Zhai1997, Lin 1999).","suffix":" Neural language model (word emb"}]}]}
>```
>%%
>*%%PREFIX%%(see, e.g., Zhai1997, Lin 1999).%%HIGHLIGHT%% ==14.3.3.2== %%POSTFIX%%Neural language model (word emb*
>%%LINK%%[[#^wrcv014bq3q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wrcv014bq3q


>%%
>```annotation-json
>{"created":"2024-09-09T13:34:38.500Z","text":"Read intro","updated":"2024-09-09T13:34:38.500Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":729439,"end":729453},{"type":"TextQuoteSelector","exact":"Topic Analysis","prefix":"erhaps fromexisting summaries?17","suffix":"This chapter is about topic mini"}]}]}
>```
>%%
>*%%PREFIX%%erhaps fromexisting summaries?17%%HIGHLIGHT%% ==Topic Analysis== %%POSTFIX%%This chapter is about topic mini*
>%%LINK%%[[#^yo8gdvwald|show annotation]]
>%%COMMENT%%
>Read intro
>%%TAGS%%
>
^yo8gdvwald


>%%
>```annotation-json
>{"created":"2024-09-09T13:35:05.456Z","updated":"2024-09-09T13:35:05.456Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":866618,"end":866622},{"type":"TextQuoteSelector","exact":"17.6","prefix":" also tend to look very similar.","suffix":" Evaluating Topic AnalysisTopic "}]}]}
>```
>%%
>*%%PREFIX%%also tend to look very similar.%%HIGHLIGHT%% ==17.6== %%POSTFIX%%Evaluating Topic AnalysisTopic*
>%%LINK%%[[#^rhnjfev1ic|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rhnjfev1ic


>%%
>```annotation-json
>{"created":"2024-09-09T13:36:10.419Z","updated":"2024-09-09T13:36:10.419Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":662996,"end":663000},{"type":"TextQuoteSelector","exact":"15.1","prefix":"mendations.15Text Categorization","suffix":" IntroductionIn the previous cha"}]}]}
>```
>%%
>*%%PREFIX%%mendations.15Text Categorization%%HIGHLIGHT%% ==15.1== %%POSTFIX%%IntroductionIn the previous cha*
>%%LINK%%[[#^6xkqqr9c0ce|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^6xkqqr9c0ce


>%%
>```annotation-json
>{"created":"2024-09-09T13:36:19.867Z","updated":"2024-09-09T13:36:19.867Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":666996,"end":667000},{"type":"TextQuoteSelector","exact":"15.2","prefix":"assages, or collections of text.","suffix":" Overview of Text Categorization"}]}]}
>```
>%%
>*%%PREFIX%%assages, or collections of text.%%HIGHLIGHT%% ==15.2== %%POSTFIX%%Overview of Text Categorization*
>%%LINK%%[[#^fqapanw89h9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^fqapanw89h9


>%%
>```annotation-json
>{"created":"2024-09-09T13:36:29.770Z","updated":"2024-09-09T13:36:29.770Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":672035,"end":672039},{"type":"TextQuoteSelector","exact":"15.3","prefix":"t from search engine evaluation.","suffix":" Text Categorization ProblemLet’"}]}]}
>```
>%%
>*%%PREFIX%%t from search engine evaluation.%%HIGHLIGHT%% ==15.3== %%POSTFIX%%Text Categorization ProblemLet’*
>%%LINK%%[[#^uykq3xd62k|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^uykq3xd62k



>%%
>```annotation-json
>{"created":"2024-09-09T13:37:06.271Z","text":"Read until \"In this case, we can tokenize documents with both unigram and bigram words.\"","updated":"2024-09-09T13:37:06.271Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":675541,"end":675545},{"type":"TextQuoteSelector","exact":"15.4","prefix":"rainfall or not, Y = { yes, no}.","suffix":" Features for Text Categorizatio"}]}]}
>```
>%%
>*%%PREFIX%%rainfall or not, Y = { yes, no}.%%HIGHLIGHT%% ==15.4== %%POSTFIX%%Features for Text Categorizatio*
>%%LINK%%[[#^m6mmbdo4cm|show annotation]]
>%%COMMENT%%
>Read until "In this case, we can tokenize documents with both unigram and bigram words."
>%%TAGS%%
>
^m6mmbdo4cm


>%%
>```annotation-json
>{"created":"2024-09-09T13:38:13.864Z","updated":"2024-09-09T13:38:13.864Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":682881,"end":682885},{"type":"TextQuoteSelector","exact":"15.5","prefix":"processing time and memoryusage.","suffix":" Classification AlgorithmsIn thi"}]}]}
>```
>%%
>*%%PREFIX%%processing time and memoryusage.%%HIGHLIGHT%% ==15.5== %%POSTFIX%%Classification AlgorithmsIn thi*
>%%LINK%%[[#^750e3cyp4ah|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^750e3cyp4ah


>%%
>```annotation-json
>{"created":"2024-09-09T13:38:17.829Z","text":"Read until \"... the class with a tie would be chosen.\"","updated":"2024-09-09T13:38:17.829Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":683948,"end":683954},{"type":"TextQuoteSelector","exact":"15.5.1","prefix":"iefin the classifier’s accuracy.","suffix":" k-Nearest Neighborsk-NN is a le"}]}]}
>```
>%%
>*%%PREFIX%%iefin the classifier’s accuracy.%%HIGHLIGHT%% ==15.5.1== %%POSTFIX%%k-Nearest Neighborsk-NN is a le*
>%%LINK%%[[#^xy3ifvawzob|show annotation]]
>%%COMMENT%%
>Read until "... the class with a tie would be chosen."
>%%TAGS%%
>
^xy3ifvawzob


>%%
>```annotation-json
>{"created":"2024-09-09T13:38:23.967Z","updated":"2024-09-09T13:38:23.967Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":684856,"end":684870},{"type":"TextQuoteSelector","exact":"Algorithm 15.1","prefix":"8 Chapter 15 Text Categorization","suffix":" k-NN TrainingCreate an inverted"}]}]}
>```
>%%
>*%%PREFIX%%8 Chapter 15 Text Categorization%%HIGHLIGHT%% ==Algorithm 15.1== %%POSTFIX%%k-NN TrainingCreate an inverted*
>%%LINK%%[[#^rfi4rt7feva|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rfi4rt7feva


>%%
>```annotation-json
>{"created":"2024-09-09T13:38:26.664Z","updated":"2024-09-09T13:38:26.664Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":684936,"end":684950},{"type":"TextQuoteSelector","exact":"Algorithm 15.2","prefix":"ndex over the training documents","suffix":" k-NN TestingLet R be the result"}]}]}
>```
>%%
>*%%PREFIX%%ndex over the training documents%%HIGHLIGHT%% ==Algorithm 15.2== %%POSTFIX%%k-NN TestingLet R be the result*
>%%LINK%%[[#^gwqf44o4rtv|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gwqf44o4rtv


>%%
>```annotation-json
>{"created":"2024-09-09T13:38:43.947Z","text":"Read until \"In this case, we can smooth the estimated probabilities using any smoothing method we’d\"","updated":"2024-09-09T13:38:43.947Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":687643,"end":687649},{"type":"TextQuoteSelector","exact":"15.5.2","prefix":" overall the training documents.","suffix":" Naive BayesNaive Bayes is an ex"}]}]}
>```
>%%
>*%%PREFIX%%overall the training documents.%%HIGHLIGHT%% ==15.5.2== %%POSTFIX%%Naive BayesNaive Bayes is an ex*
>%%LINK%%[[#^627whnf1ws6|show annotation]]
>%%COMMENT%%
>Read until "In this case, we can smooth the estimated probabilities using any smoothing method we’d"
>%%TAGS%%
>
^627whnf1ws6


>%%
>```annotation-json
>{"created":"2024-09-09T13:40:44.093Z","updated":"2024-09-09T13:40:44.093Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":696544,"end":696548},{"type":"TextQuoteSelector","exact":"15.6","prefix":"nto totals for each class label.","suffix":" Evaluation of Text Categorizati"}]}]}
>```
>%%
>*%%PREFIX%%nto totals for each class label.%%HIGHLIGHT%% ==15.6== %%POSTFIX%%Evaluation of Text Categorizati*
>%%LINK%%[[#^5c05u3my9fp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5c05u3my9fp


>%%
>```annotation-json
>{"created":"2024-09-09T13:48:31.209Z","updated":"2024-09-09T13:48:31.209Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":613711,"end":613726},{"type":"TextQuoteSelector","exact":"Intuitively, we","prefix":" to be in a particular cluster? ","suffix":"276 Chapter 14 Text ClusteringFi"}]}]}
>```
>%%
>*%%PREFIX%%to be in a particular cluster?%%HIGHLIGHT%% ==Intuitively, we== %%POSTFIX%%276 Chapter 14 Text ClusteringFi*
>%%LINK%%[[#^zooi0crqzbq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zooi0crqzbq


>%%
>```annotation-json
>{"created":"2024-09-09T13:49:05.112Z","updated":"2024-09-09T13:49:05.112Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":614194,"end":614610},{"type":"TextQuoteSelector","exact":"imagine objects inside the same cluster are similar in some way—more so than ob-jectsthatappearintwodifferentclusters.However, suchadefinitionofclusteringisstrictly speaking not well defined as we did not make it clear how exactly we shouldmeasure similarity. Indeed, an appropriate definition of similarity is quite crucialfor clustering as a different definition would clearly lead to a different clusteringresult.","prefix":"larity is defined based on size.","suffix":"Consider the illustration in Fig"}]}]}
>```
>%%
>*%%PREFIX%%larity is defined based on size.%%HIGHLIGHT%% ==imagine objects inside the same cluster are similar in some way—more so than ob-jectsthatappearintwodifferentclusters.However, suchadefinitionofclusteringisstrictly speaking not well defined as we did not make it clear how exactly we shouldmeasure similarity. Indeed, an appropriate definition of similarity is quite crucialfor clustering as a different definition would clearly lead to a different clusteringresult.== %%POSTFIX%%Consider the illustration in Fig*
>%%LINK%%[[#^3hexo03e3ed|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3hexo03e3ed


>%%
>```annotation-json
>{"created":"2024-09-09T13:49:56.525Z","updated":"2024-09-09T13:49:56.525Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":615223,"end":615376},{"type":"TextQuoteSelector","exact":"Thus, when we define a clusteringtask, it is important to state the desired perspective of measuring similarity, whichwe refer to as a “clustering bias.”","prefix":"n the figure on the right side. ","suffix":" This bias will also be the basi"}]}]}
>```
>%%
>*%%PREFIX%%n the figure on the right side.%%HIGHLIGHT%% ==Thus, when we define a clusteringtask, it is important to state the desired perspective of measuring similarity, whichwe refer to as a “clustering bias.”== %%POSTFIX%%This bias will also be the basi*
>%%LINK%%[[#^n1xwekjsu|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^n1xwekjsu


>%%
>```annotation-json
>{"created":"2024-09-09T13:50:47.581Z","updated":"2024-09-09T13:50:47.581Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":615907,"end":616122},{"type":"TextQuoteSelector","exact":"In different algorithms, the clustering bias is injected in different ways. Forsome clustering algorithms, it is up to the user to define or select explicitly asimilarity algorithm for the clustering method to use. ","prefix":"ned by the specific application.","suffix":"It will put (for example)documen"}]}]}
>```
>%%
>*%%PREFIX%%ned by the specific application.%%HIGHLIGHT%% ==In different algorithms, the clustering bias is injected in different ways. Forsome clustering algorithms, it is up to the user to define or select explicitly asimilarity algorithm for the clustering method to use.== %%POSTFIX%%It will put (for example)documen*
>%%LINK%%[[#^pukb8pxnnrf|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pukb8pxnnrf


>%%
>```annotation-json
>{"created":"2024-09-09T13:56:50.992Z","updated":"2024-09-09T13:56:50.992Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":618277,"end":618431},{"type":"TextQuoteSelector","exact":"For both above methods, each document can only belong to one cluster. Thisis a “hard assignment,” unlike the clusters we receive from a model-basedmethod.","prefix":"f objects into smaller clusters.","suffix":"Model-basedtechniques design a p"}]}]}
>```
>%%
>*%%PREFIX%%f objects into smaller clusters.%%HIGHLIGHT%% ==For both above methods, each document can only belong to one cluster. Thisis a “hard assignment,” unlike the clusters we receive from a model-basedmethod.== %%POSTFIX%%Model-basedtechniques design a p*
>%%LINK%%[[#^s4734ekxff|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^s4734ekxff


>%%
>```annotation-json
>{"created":"2024-09-09T14:05:04.715Z","updated":"2024-09-09T14:05:04.715Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":618825,"end":619146},{"type":"TextQuoteSelector","exact":"Term clustering has applications in query expansion. It allows similar terms tobe added to the query, increasing the possible number of documents matched fromthe index. It also allows humans to understand advanced features more easily ifthere are many hundreds or thousands of them, or if they are hard to conceptual-ize.","prefix":"r.278 Chapter 14 Text Clustering","suffix":" Later, we will first discuss te"}]}]}
>```
>%%
>*%%PREFIX%%r.278 Chapter 14 Text Clustering%%HIGHLIGHT%% ==Term clustering has applications in query expansion. It allows similar terms tobe added to the query, increasing the possible number of documents matched fromthe index. It also allows humans to understand advanced features more easily ifthere are many hundreds or thousands of them, or if they are hard to conceptual-ize.== %%POSTFIX%%Later, we will first discuss te*
>%%LINK%%[[#^toxwvk50g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^toxwvk50g


>%%
>```annotation-json
>{"created":"2024-09-09T14:07:28.146Z","updated":"2024-09-09T14:07:28.146Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":619964,"end":620450},{"type":"TextQuoteSelector","exact":"Although with an unsupervised clustering algorithm, we generally do not provideany prior expectations as to what our clusters may contain, it is also possible toprovide some supervision in the form of requiring two objects to be in the samecluster or not to be in the same cluster. Such supervision is useful when we havesome knowledge about the clustering problem that we would like to incorporateinto a clustering algorithm and allows users to “steer” the clustering in a flexibleway.","prefix":"d document clusters to the user.","suffix":" A user can also control the num"}]}]}
>```
>%%
>*%%PREFIX%%d document clusters to the user.%%HIGHLIGHT%% ==Although with an unsupervised clustering algorithm, we generally do not provideany prior expectations as to what our clusters may contain, it is also possible toprovide some supervision in the form of requiring two objects to be in the samecluster or not to be in the same cluster. Such supervision is useful when we havesome knowledge about the clustering problem that we would like to incorporateinto a clustering algorithm and allows users to “steer” the clustering in a flexibleway.== %%POSTFIX%%A user can also control the num*
>%%LINK%%[[#^1yd20o3lwqh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1yd20o3lwqh



>%%
>```annotation-json
>{"created":"2024-09-09T14:08:35.679Z","updated":"2024-09-09T14:08:35.679Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":620815,"end":621137},{"type":"TextQuoteSelector","exact":"Most clustering output does not give labels for the clusters found; it’s up to theuser to examine the groups of terms or documents and mentally assign a labelsuchas“biology”or“architecture.”However, therearealsoapproachestoautomateassignment of a label to a text cluster where a label is often a phrase or multiplephrases ","prefix":"by a certain number of clusters.","suffix":"[Mei et al. 2007b]. This labelin"}]}]}
>```
>%%
>*%%PREFIX%%by a certain number of clusters.%%HIGHLIGHT%% ==Most clustering output does not give labels for the clusters found; it’s up to theuser to examine the groups of terms or documents and mentally assign a labelsuchas“biology”or“architecture.”However, therearealsoapproachestoautomateassignment of a label to a text cluster where a label is often a phrase or multiplephrases== %%POSTFIX%%[Mei et al. 2007b]. This labelin*
>%%LINK%%[[#^1drakw4u5k1|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1drakw4u5k1


>%%
>```annotation-json
>{"created":"2024-09-09T14:09:29.437Z","updated":"2024-09-09T14:09:29.437Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":621415,"end":621582},{"type":"TextQuoteSelector","exact":" information retrievaltechniques that we discussed in Part II are often also very useful for implementingmany other algorithms for text analysis, including clustering.","prefix":"f the book, we will see that the","suffix":" For example, in thecase of docu"}]}]}
>```
>%%
>*%%PREFIX%%f the book, we will see that the%%HIGHLIGHT%% ==information retrievaltechniques that we discussed in Part II are often also very useful for implementingmany other algorithms for text analysis, including clustering.== %%POSTFIX%%For example, in thecase of docu*
>%%LINK%%[[#^hb4qzblejh8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hb4qzblejh8



>%%
>```annotation-json
>{"created":"2024-09-09T14:12:27.931Z","updated":"2024-09-09T14:12:27.931Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":622997,"end":623335},{"type":"TextQuoteSelector","exact":"In particular, the similarity algorithms we use for clustering need to be symmet-ric; that is, sim(d1, d2) must be equal to sim(d2, d1). Furthermore, our similarityalgorithm must be normalized on some range. Usually, this range is [0, 1]. Theseconstraints ensure that we can fairly compare similarity scores of different pairs ofobjects. ","prefix":"st the reader consult Chapter 6.","suffix":"Most retrieval formulas we have "}]}]}
>```
>%%
>*%%PREFIX%%st the reader consult Chapter 6.%%HIGHLIGHT%% ==In particular, the similarity algorithms we use for clustering need to be symmet-ric; that is, sim(d1, d2) must be equal to sim(d2, d1). Furthermore, our similarityalgorithm must be normalized on some range. Usually, this range is [0, 1]. Theseconstraints ensure that we can fairly compare similarity scores of different pairs ofobjects.== %%POSTFIX%%Most retrieval formulas we have*
>%%LINK%%[[#^9n4m2l3wfru|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^9n4m2l3wfru


>%%
>```annotation-json
>{"created":"2024-09-09T14:15:13.038Z","updated":"2024-09-09T14:15:13.038Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":624267,"end":624366},{"type":"TextQuoteSelector","exact":"The cosine similar-ity captures the cosine of the angle between the two document vectors plotted in","prefix":"ng else the user could imagine. ","suffix":"280 Chapter 14 Text Clusteringth"}]}]}
>```
>%%
>*%%PREFIX%%ng else the user could imagine.%%HIGHLIGHT%% ==The cosine similar-ity captures the cosine of the angle between the two document vectors plotted in== %%POSTFIX%%280 Chapter 14 Text Clusteringth*
>%%LINK%%[[#^blx9djcw294|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^blx9djcw294


>%%
>```annotation-json
>{"created":"2024-09-09T14:15:18.740Z","updated":"2024-09-09T14:15:18.740Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":624396,"end":624487},{"type":"TextQuoteSelector","exact":"their high-dimensional space; the larger the angle, the more dissimilar the docu-ments are.","prefix":"in280 Chapter 14 Text Clustering","suffix":"Another common similarity metric"}]}]}
>```
>%%
>*%%PREFIX%%in280 Chapter 14 Text Clustering%%HIGHLIGHT%% ==their high-dimensional space; the larger the angle, the more dissimilar the docu-ments are.== %%POSTFIX%%Another common similarity metric*
>%%LINK%%[[#^pfok07ljna9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pfok07ljna9


>%%
>```annotation-json
>{"created":"2024-09-09T14:15:45.051Z","updated":"2024-09-09T14:15:45.051Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":624523,"end":624542},{"type":"TextQuoteSelector","exact":"Jaccard similarity.","prefix":"her common similarity metric is ","suffix":" This metric is a setsimilarity;"}]}]}
>```
>%%
>*%%PREFIX%%her common similarity metric is%%HIGHLIGHT%% ==Jaccard similarity.== %%POSTFIX%%This metric is a setsimilarity;*
>%%LINK%%[[#^j14xcyhi4i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^j14xcyhi4i


>%%
>```annotation-json
>{"created":"2024-09-09T14:15:52.542Z","updated":"2024-09-09T14:15:52.542Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":624835,"end":624898},{"type":"TextQuoteSelector","exact":", itcapturestheratioofsharedobjectsandtotalobjectsin both sets.","prefix":"nd y,respectively.InplainEnglish","suffix":"For a more in-depth study of sim"}]}]}
>```
>%%
>*%%PREFIX%%nd y,respectively.InplainEnglish%%HIGHLIGHT%% ==, itcapturestheratioofsharedobjectsandtotalobjectsin both sets.== %%POSTFIX%%For a more in-depth study of sim*
>%%LINK%%[[#^a27u5kie72a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^a27u5kie72a


>%%
>```annotation-json
>{"created":"2024-09-09T14:16:23.844Z","updated":"2024-09-09T14:16:23.844Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":625163,"end":625350},{"type":"TextQuoteSelector","exact":"In any event, the goal of a particular similarity algorithm is to findan optimal partitioning of data to simultaneously maximize intra-group similarityand minimize inter-group similarity.","prefix":" cosine or Jac-card similarity. ","suffix":"14.2.1 Agglomerative Hierarchica"}]}]}
>```
>%%
>*%%PREFIX%%cosine or Jac-card similarity.%%HIGHLIGHT%% ==In any event, the goal of a particular similarity algorithm is to findan optimal partitioning of data to simultaneously maximize intra-group similarityand minimize inter-group similarity.== %%POSTFIX%%14.2.1 Agglomerative Hierarchica*
>%%LINK%%[[#^02zmbipnuga7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^02zmbipnuga7


>%%
>```annotation-json
>{"created":"2024-09-09T14:17:54.530Z","updated":"2024-09-09T14:17:54.530Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":625461,"end":625731},{"type":"TextQuoteSelector","exact":"This methodprogressively constructs clusters to generate a hierarchy of merged groups. Thisbottom-up (agglomerative) approach gradually groups similar objects (single doc-uments or groups of documents) into larger and larger clusters until there is onlyone cluster left.","prefix":"st general clustering strategy. ","suffix":" The tree may then be segmented "}]}]}
>```
>%%
>*%%PREFIX%%st general clustering strategy.%%HIGHLIGHT%% ==This methodprogressively constructs clusters to generate a hierarchy of merged groups. Thisbottom-up (agglomerative) approach gradually groups similar objects (single doc-uments or groups of documents) into larger and larger clusters until there is onlyone cluster left.== %%POSTFIX%%The tree may then be segmented*
>%%LINK%%[[#^o2m6sx81f49|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^o2m6sx81f49


>%%
>```annotation-json
>{"created":"2024-09-09T14:20:13.369Z","updated":"2024-09-09T14:20:13.369Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":626201,"end":626335},{"type":"TextQuoteSelector","exact":"The clustering algorithm is straightforward: while there is more than one clus-ter, find the two most similar clusters and merge them.","prefix":"o form the next, larger cluster.","suffix":" This does present an issuethoug"}]}]}
>```
>%%
>*%%PREFIX%%o form the next, larger cluster.%%HIGHLIGHT%% ==The clustering algorithm is straightforward: while there is more than one clus-ter, find the two most similar clusters and merge them.== %%POSTFIX%%This does present an issuethoug*
>%%LINK%%[[#^w7wyo7ivuce|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^w7wyo7ivuce


>%%
>```annotation-json
>{"created":"2024-09-09T14:20:56.535Z","updated":"2024-09-09T14:20:56.535Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":626688,"end":626804},{"type":"TextQuoteSelector","exact":"The cluster similarity measures we define make use of the document-documentsimilarity measures presented previously.","prefix":"compare clusters for similarity.","suffix":"14.2 Document Clustering 2811 2 "}]}]}
>```
>%%
>*%%PREFIX%%compare clusters for similarity.%%HIGHLIGHT%% ==The cluster similarity measures we define make use of the document-documentsimilarity measures presented previously.== %%POSTFIX%%14.2 Document Clustering 2811 2*
>%%LINK%%[[#^bp3x572phnm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bp3x572phnm


>%%
>```annotation-json
>{"created":"2024-09-09T14:22:32.457Z","updated":"2024-09-09T14:22:32.457Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":628045,"end":628252},{"type":"TextQuoteSelector","exact":"Both single-link and complete-link are sensitive to outliers since they rely on onlythe similarity of one pair of documents. Average-link is essentially a group decision,making it less sensitive to outliers.","prefix":"e distance between two clusters.","suffix":" Of course, as with most methods"}]}]}
>```
>%%
>*%%PREFIX%%e distance between two clusters.%%HIGHLIGHT%% ==Both single-link and complete-link are sensitive to outliers since they rely on onlythe similarity of one pair of documents. Average-link is essentially a group decision,making it less sensitive to outliers.== %%POSTFIX%%Of course, as with most methods*
>%%LINK%%[[#^vemxrlrcmlm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vemxrlrcmlm


>%%
>```annotation-json
>{"created":"2024-09-09T14:24:00.111Z","updated":"2024-09-09T14:24:00.111Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":628672,"end":629108},{"type":"TextQuoteSelector","exact":"In this approach, we repeatedly apply a flat clustering algorithmto partition the data into smaller and smaller clusters. In flat clustering, We willstart with an initial tentative clustering and iteratively improve it until we reachsome stopping criterion. Here, we represent a cluster with a centroid; a centroid isa special document that represents all other documents in its cluster, usually as anaverage of all its members’ values.","prefix":"s a top-down,divisive approach. ","suffix":"The K-means algorithm1 sets K ce"}]}]}
>```
>%%
>*%%PREFIX%%s a top-down,divisive approach.%%HIGHLIGHT%% ==In this approach, we repeatedly apply a flat clustering algorithmto partition the data into smaller and smaller clusters. In flat clustering, We willstart with an initial tentative clustering and iteratively improve it until we reachsome stopping criterion. Here, we represent a cluster with a centroid; a centroid isa special document that represents all other documents in its cluster, usually as anaverage of all its members’ values.== %%POSTFIX%%The K-means algorithm1 sets K ce*
>%%LINK%%[[#^n8n3z498jwo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^n8n3z498jwo


>%%
>```annotation-json
>{"created":"2024-09-09T14:27:45.392Z","updated":"2024-09-09T14:27:45.392Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":629372,"end":629586},{"type":"TextQuoteSelector","exact":"The two steps in K-means are marked as the expectation step (Ex.) and themaximization step (Max.); this algorithm is one instantiation of the widely foundExpectation-Maximization algorithm, commonly called just EM.","prefix":"ent-document similarity measure.","suffix":" We will return tothis powerful "}]}]}
>```
>%%
>*%%PREFIX%%ent-document similarity measure.%%HIGHLIGHT%% ==The two steps in K-means are marked as the expectation step (Ex.) and themaximization step (Max.); this algorithm is one instantiation of the widely foundExpectation-Maximization algorithm, commonly called just EM.== %%POSTFIX%%We will return tothis powerful*
>%%LINK%%[[#^0ci0ysy97zm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0ci0ysy97zm


>%%
>```annotation-json
>{"created":"2024-09-09T14:28:37.939Z","updated":"2024-09-09T14:28:37.939Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":632226,"end":632419},{"type":"TextQuoteSelector","exact":"The algorithm is also known toconverge to a local minimum, but not guaranteed to converge to a global minimum.Thus, multiple trials are generally needed in order to obtain a good local minimum.","prefix":"to the centroid of the cluster. ","suffix":"TheK-meansalgorithmcanberepeated"}]}]}
>```
>%%
>*%%PREFIX%%to the centroid of the cluster.%%HIGHLIGHT%% ==The algorithm is also known toconverge to a local minimum, but not guaranteed to converge to a global minimum.Thus, multiple trials are generally needed in order to obtain a good local minimum.== %%POSTFIX%%TheK-meansalgorithmcanberepeated*
>%%LINK%%[[#^yjnykd8q4dc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^yjnykd8q4dc



>%%
>```annotation-json
>{"created":"2024-09-09T14:30:31.727Z","updated":"2024-09-09T14:30:31.727Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":632419,"end":633370},{"type":"TextQuoteSelector","exact":"TheK-meansalgorithmcanberepeatedlyappliedtodividethedatasetgraduallyinto smaller and smaller clusters, thus creating a hierarchy of clusters similarto what we can achieve with the agglomerative hierarchical clustering algorithm.Thus both agglomerative hierarchical clustering and K-means can be used forhierarchical clustering; they complement each other in the sense that K-meansconstructs the hierarchy by incrementally dividing the whole set of data (a top-downstrategy), while agglomerative hierarchical clustering constructs the hierarchy byincrementally merging data points (a bottom-up strategy). Note that although inits basic form, agglomerative hierarchical clustering generates a binary tree, it caneasily adapted to generate more than two branches by merging more than twogroups into a cluster at each iteration. Similarly, if we only allow a binary tree, thenwe also do not have to set K in the K-means algorithm for creating a hierarchy.","prefix":" to obtain a good local minimum.","suffix":"14.3 Term ClusteringThe goal of "}]}]}
>```
>%%
>*%%PREFIX%%to obtain a good local minimum.%%HIGHLIGHT%% ==TheK-meansalgorithmcanberepeatedlyappliedtodividethedatasetgraduallyinto smaller and smaller clusters, thus creating a hierarchy of clusters similarto what we can achieve with the agglomerative hierarchical clustering algorithm.Thus both agglomerative hierarchical clustering and K-means can be used forhierarchical clustering; they complement each other in the sense that K-meansconstructs the hierarchy by incrementally dividing the whole set of data (a top-downstrategy), while agglomerative hierarchical clustering constructs the hierarchy byincrementally merging data points (a bottom-up strategy). Note that although inits basic form, agglomerative hierarchical clustering generates a binary tree, it caneasily adapted to generate more than two branches by merging more than twogroups into a cluster at each iteration. Similarly, if we only allow a binary tree, thenwe also do not have to set K in the K-means algorithm for creating a hierarchy.== %%POSTFIX%%14.3 Term ClusteringThe goal of*
>%%LINK%%[[#^chfpkpvr3r5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^chfpkpvr3r5



>%%
>```annotation-json
>{"created":"2024-09-09T14:50:35.953Z","updated":"2024-09-09T14:50:35.953Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":648317,"end":648664},{"type":"TextQuoteSelector","exact":"uch a contextual view ofterm representation can not only be used for discovering paradigmatic relations,but also support term clustering in general since we can use any document cluster-ing algorithm by viewing a term as a “document” represented by a vector. It can alsohelp word sense disambiguation since when an ambiguous word takes a different","prefix":" their vector representations. S","suffix":"292 Chapter 14 Text Clusteringse"}]}]}
>```
>%%
>*%%PREFIX%%their vector representations. S%%HIGHLIGHT%% ==uch a contextual view ofterm representation can not only be used for discovering paradigmatic relations,but also support term clustering in general since we can use any document cluster-ing algorithm by viewing a term as a “document” represented by a vector. It can alsohelp word sense disambiguation since when an ambiguous word takes a different== %%POSTFIX%%292 Chapter 14 Text Clusteringse*
>%%LINK%%[[#^x6bs8fhcnw|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^x6bs8fhcnw


>%%
>```annotation-json
>{"created":"2024-09-09T14:50:43.739Z","updated":"2024-09-09T14:50:43.739Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":648694,"end":648814},{"type":"TextQuoteSelector","exact":"sense, it tends to “attract” different words in its surrounding text, thus would havea different context representation.","prefix":"nt292 Chapter 14 Text Clustering","suffix":"This technique is not limited to"}]}]}
>```
>%%
>*%%PREFIX%%nt292 Chapter 14 Text Clustering%%HIGHLIGHT%% ==sense, it tends to “attract” different words in its surrounding text, thus would havea different context representation.== %%POSTFIX%%This technique is not limited to*
>%%LINK%%[[#^2wtsy1i9ahy|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2wtsy1i9ahy


>%%
>```annotation-json
>{"created":"2024-09-09T14:53:16.639Z","updated":"2024-09-09T14:53:16.639Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":649286,"end":649640},{"type":"TextQuoteSelector","exact":"However, the heuristic way to obtain vector representation discussed in Chap-ter 13 has the disadvantage that we need to make many ad hoc choices, especiallyin how to obtain the term weights. Another deficiency is that the vector spans theentire space of words in the vocabulary, increasing the complexity of any furtherprocessing applied to the vectors.","prefix":"statistical machine translation.","suffix":"As an alternative, we can use a "}]}]}
>```
>%%
>*%%PREFIX%%statistical machine translation.%%HIGHLIGHT%% ==However, the heuristic way to obtain vector representation discussed in Chap-ter 13 has the disadvantage that we need to make many ad hoc choices, especiallyin how to obtain the term weights. Another deficiency is that the vector spans theentire space of words in the vocabulary, increasing the complexity of any furtherprocessing applied to the vectors.== %%POSTFIX%%As an alternative, we can use a*
>%%LINK%%[[#^r6lyb622czr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^r6lyb622czr


>%%
>```annotation-json
>{"created":"2024-09-09T14:54:03.942Z","updated":"2024-09-09T14:54:03.942Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":654068,"end":654072},{"type":"TextQuoteSelector","exact":"14.4","prefix":"ity to “match” with each other).","suffix":" Evaluation of Text ClusteringAl"}]}]}
>```
>%%
>*%%PREFIX%%ity to “match” with each other).%%HIGHLIGHT%% ==14.4== %%POSTFIX%%Evaluation of Text ClusteringAl*
>%%LINK%%[[#^hate4i9ihzf|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hate4i9ihzf


>%%
>```annotation-json
>{"created":"2024-09-09T14:54:40.546Z","updated":"2024-09-09T14:54:40.546Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":649640,"end":649966},{"type":"TextQuoteSelector","exact":"As an alternative, we can use a neural language model [Mikolov et al. 2010] to sys-tematically learn a vector representation for each word by optimizing a meaningfulobjective function. Such an approach is also called word embedding, which refers tothe mapping of a word into a vector representation in a low-dimensional space.","prefix":"ocessing applied to the vectors.","suffix":" Thegeneral idea of these method"}]}]}
>```
>%%
>*%%PREFIX%%ocessing applied to the vectors.%%HIGHLIGHT%% ==As an alternative, we can use a neural language model [Mikolov et al. 2010] to sys-tematically learn a vector representation for each word by optimizing a meaningfulobjective function. Such an approach is also called word embedding, which refers tothe mapping of a word into a vector representation in a low-dimensional space.== %%POSTFIX%%Thegeneral idea of these method*
>%%LINK%%[[#^tbwnj6i5p3|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^tbwnj6i5p3


>%%
>```annotation-json
>{"created":"2024-09-09T15:00:47.041Z","updated":"2024-09-09T15:00:47.041Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":650505,"end":650656},{"type":"TextQuoteSelector","exact":"For example, to model an n-gram lan-guage model p(wn | wn−1, . . . , w1), the neural network would have wn−1, . . . , w1 asinput and wn as the output. ","prefix":"epresented as a neural network. ","suffix":"In some neural language models, "}]}]}
>```
>%%
>*%%PREFIX%%epresented as a neural network.%%HIGHLIGHT%% ==For example, to model an n-gram lan-guage model p(wn | wn−1, . . . , w1), the neural network would have wn−1, . . . , w1 asinput and wn as the output.== %%POSTFIX%%In some neural language models,*
>%%LINK%%[[#^a9g4t6gtp5h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^a9g4t6gtp5h


>%%
>```annotation-json
>{"created":"2024-09-09T15:04:04.302Z","updated":"2024-09-09T15:04:04.302Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":651738,"end":651952},{"type":"TextQuoteSelector","exact":"With such a model, we canthen try to find the vector representation for all the words that would maximizethe probability of using each word to predict all other words in a small windowof words surrounding the word.","prefix":"ng to the two words, w1 and w2. ","suffix":" In effect, we would want the ve"}]}]}
>```
>%%
>*%%PREFIX%%ng to the two words, w1 and w2.%%HIGHLIGHT%% ==With such a model, we canthen try to find the vector representation for all the words that would maximizethe probability of using each word to predict all other words in a small windowof words surrounding the word.== %%POSTFIX%%In effect, we would want the ve*
>%%LINK%%[[#^9uyr2orbepe|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^9uyr2orbepe


>%%
>```annotation-json
>{"created":"2024-09-09T15:04:58.031Z","updated":"2024-09-09T15:04:58.031Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":652205,"end":652213},{"type":"TextQuoteSelector","exact":"word2vec","prefix":"ementation of skip-gram, called ","suffix":" [Mikolov et al. 2013] is perhap"}]}]}
>```
>%%
>*%%PREFIX%%ementation of skip-gram, called%%HIGHLIGHT%% ==word2vec== %%POSTFIX%%[Mikolov et al. 2013] is perhap*
>%%LINK%%[[#^xe6ors4milq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^xe6ors4milq


>%%
>```annotation-json
>{"created":"2024-09-09T15:05:06.495Z","updated":"2024-09-09T15:05:06.495Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":652237,"end":652494},{"type":"TextQuoteSelector","exact":"s perhaps the most well-known software in this area. They showed that performingvector addition on terms in vector space yielded interesting results. For example,adding the vectors for Germany and capital resulted in a vector very close to thevector Berlin.","prefix":"word2vec [Mikolov et al. 2013] i","suffix":" Figure 14.8 shows example outpu"}]}]}
>```
>%%
>*%%PREFIX%%word2vec [Mikolov et al. 2013] i%%HIGHLIGHT%% ==s perhaps the most well-known software in this area. They showed that performingvector addition on terms in vector space yielded interesting results. For example,adding the vectors for Germany and capital resulted in a vector very close to thevector Berlin.== %%POSTFIX%%Figure 14.8 shows example outpu*
>%%LINK%%[[#^keke3pcxmhq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^keke3pcxmhq


>%%
>```annotation-json
>{"created":"2024-09-09T15:05:45.953Z","updated":"2024-09-09T15:05:45.953Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":652736,"end":653076},{"type":"TextQuoteSelector","exact":"word embedding provides a very promising new alternative that can poten-tially open up many interesting new applications of text mining due to its flexibilityin formulating the objective functions to be optimized and the fact that the vectorrepresentation is systematically learned through optimizing an explicitly definedobjective function","prefix":"the n-gram class languagemodel, ","suffix":". One disadvantage of word embed"}]}]}
>```
>%%
>*%%PREFIX%%the n-gram class languagemodel,%%HIGHLIGHT%% ==word embedding provides a very promising new alternative that can poten-tially open up many interesting new applications of text mining due to its flexibilityin formulating the objective functions to be optimized and the fact that the vectorrepresentation is systematically learned through optimizing an explicitly definedobjective function== %%POSTFIX%%. One disadvantage of word embed*
>%%LINK%%[[#^zz0mnq46ern|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zz0mnq46ern


>%%
>```annotation-json
>{"created":"2024-09-09T15:09:51.553Z","updated":"2024-09-09T15:09:51.553Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":654530,"end":654696},{"type":"TextQuoteSelector","exact":"Of the three criteria mentioned above, coherenceand separation can be measured automatically with measures such as vector sim-ilarity, purity, or mutual information. ","prefix":"ion(using predefined measures). ","suffix":"There is a slight challenge when"}]}]}
>```
>%%
>*%%PREFIX%%ion(using predefined measures).%%HIGHLIGHT%% ==Of the three criteria mentioned above, coherenceand separation can be measured automatically with measures such as vector sim-ilarity, purity, or mutual information.== %%POSTFIX%%There is a slight challenge when*
>%%LINK%%[[#^6b8adz9pw9v|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^6b8adz9pw9v


>%%
>```annotation-json
>{"created":"2024-09-09T15:15:35.378Z","updated":"2024-09-09T15:15:35.378Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":659053,"end":659423},{"type":"TextQuoteSelector","exact":"Which method works the best highly depends on whether the bias (definitionof similarity) reflects our perspective for clustering accurately and whether the as-sumptions made by an approach hold for the problem and applications. In general,model-based approaches have more potential for doing “complex clustering” byencoding more constraints into the probabilistic model.","prefix":"ation needs or domain knowledge.","suffix":"Bibliographic Notes and Further "}]}]}
>```
>%%
>*%%PREFIX%%ation needs or domain knowledge.%%HIGHLIGHT%% ==Which method works the best highly depends on whether the bias (definitionof similarity) reflects our perspective for clustering accurately and whether the as-sumptions made by an approach hold for the problem and applications. In general,model-based approaches have more potential for doing “complex clustering” byencoding more constraints into the probabilistic model.== %%POSTFIX%%Bibliographic Notes and Further*
>%%LINK%%[[#^0z3bdc7buhl|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0z3bdc7buhl


>%%
>```annotation-json
>{"created":"2024-09-09T15:16:34.496Z","updated":"2024-09-09T15:16:34.496Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":656600,"end":657106},{"type":"TextQuoteSelector","exact":"As we saw in this chapter, similarity-based algorithms explicitly encode a simi-larity function in their implementation. Ideally, this similarity between objects isoptimized to maximize intra-cluster coherence and minimize intra-cluster separa-tion. In model-based methods (which will be discussed in Chapter 17), similarityfunctions are not inherently part of the model; instead, the notion of object sim-ilarity is most often captured by probabilistically high co-occurring terms within“similar” objects.","prefix":"ver-all classification accuracy.","suffix":"Measuring coherence and separati"}]}]}
>```
>%%
>*%%PREFIX%%ver-all classification accuracy.%%HIGHLIGHT%% ==As we saw in this chapter, similarity-based algorithms explicitly encode a simi-larity function in their implementation. Ideally, this similarity between objects isoptimized to maximize intra-cluster coherence and minimize intra-cluster separa-tion. In model-based methods (which will be discussed in Chapter 17), similarityfunctions are not inherently part of the model; instead, the notion of object sim-ilarity is most often captured by probabilistically high co-occurring terms within“similar” objects.== %%POSTFIX%%Measuring coherence and separati*
>%%LINK%%[[#^eckw4mmyl1w|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^eckw4mmyl1w


>%%
>```annotation-json
>{"created":"2024-09-11T09:34:46.747Z","updated":"2024-09-11T09:34:46.747Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":664261,"end":664357},{"type":"TextQuoteSelector","exact":"At the very high level, text categorization is usually to help achieve two goals ofapplications.","prefix":"isis illustrated in Figure 15.1.","suffix":"1. To enrich text representation"}]}]}
>```
>%%
>*%%PREFIX%%isis illustrated in Figure 15.1.%%HIGHLIGHT%% ==At the very high level, text categorization is usually to help achieve two goals ofapplications.== %%POSTFIX%%1. To enrich text representation*
>%%LINK%%[[#^qf9quxq1eq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qf9quxq1eq


>%%
>```annotation-json
>{"created":"2024-09-11T09:35:27.012Z","updated":"2024-09-11T09:35:27.012Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":664360,"end":664389},{"type":"TextQuoteSelector","exact":"To enrich text representation","prefix":"eve two goals ofapplications.1. ","suffix":" (i.e., achieving more understan"}]}]}
>```
>%%
>*%%PREFIX%%eve two goals ofapplications.1.%%HIGHLIGHT%% ==To enrich text representation== %%POSTFIX%%(i.e., achieving more understan*
>%%LINK%%[[#^yc20fkpb5km|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^yc20fkpb5km


>%%
>```annotation-json
>{"created":"2024-09-11T09:35:32.280Z","updated":"2024-09-11T09:35:32.280Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":664535,"end":664601},{"type":"TextQuoteSelector","exact":".Insuchanapplication, wealsocalltextcategorizationtext annotation.","prefix":"iple levels(keywords+categories)","suffix":" For example, semantic categorie"}]}]}
>```
>%%
>*%%PREFIX%%iple levels(keywords+categories)%%HIGHLIGHT%% ==.Insuchanapplication, wealsocalltextcategorizationtext annotation.== %%POSTFIX%%For example, semantic categorie*
>%%LINK%%[[#^sgpkjw497ga|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sgpkjw497ga


>%%
>```annotation-json
>{"created":"2024-09-11T09:37:31.472Z","updated":"2024-09-11T09:37:31.472Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":665264,"end":665367},{"type":"TextQuoteSelector","exact":"To infer properties of entities associated with text data (i.e., discovery ofknowledge about the world)","prefix":"training examples available).2. ","suffix":": as long as an entity can be as"}]}]}
>```
>%%
>*%%PREFIX%%training examples available).2.%%HIGHLIGHT%% ==To infer properties of entities associated with text data (i.e., discovery ofknowledge about the world)== %%POSTFIX%%: as long as an entity can be as*
>%%LINK%%[[#^8z5mrnldeec|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^8z5mrnldeec


>%%
>```annotation-json
>{"created":"2024-09-11T09:37:40.233Z","updated":"2024-09-11T09:37:40.233Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":665533,"end":665661},{"type":"TextQuoteSelector","exact":"For example, we can use the English textdata written by a person to predict whether the person is a non-native speakerof English","prefix":"gorize the associated entities. ","suffix":". Prediction of party affiliatio"}]}]}
>```
>%%
>*%%PREFIX%%gorize the associated entities.%%HIGHLIGHT%% ==For example, we can use the English textdata written by a person to predict whether the person is a non-native speakerof English== %%POSTFIX%%. Prediction of party affiliatio*
>%%LINK%%[[#^0zlef33zt6ea|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0zlef33zt6ea


>%%
>```annotation-json
>{"created":"2024-09-11T09:37:59.408Z","updated":"2024-09-11T09:37:59.408Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":665876,"end":665972},{"type":"TextQuoteSelector","exact":"Indeed,in such an application, text categorization should really be called text-basedprediction.","prefix":"gory and text content is large. ","suffix":"These two somewhat different goa"}]}]}
>```
>%%
>*%%PREFIX%%gory and text content is large.%%HIGHLIGHT%% ==Indeed,in such an application, text categorization should really be called text-basedprediction.== %%POSTFIX%%These two somewhat different goa*
>%%LINK%%[[#^jzxl37r2ps|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jzxl37r2ps


>%%
>```annotation-json
>{"created":"2024-09-11T09:39:27.671Z","updated":"2024-09-11T09:39:27.671Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":666088,"end":666577},{"type":"TextQuoteSelector","exact":"For the purpose of enriching text rep-resentation, the categories tend to be “internal” categories that characterize a textobject (e.g., topical categories, sentiment categories). For the purpose of inferringproperties of associated entities with text data, the categories tend to be “external”categories that characterize an entity associated with the text object (e.g., author at-tribution or any other meaningful categories associated with text data, potentiallythrough indirect links).","prefix":"in the categories in each case. ","suffix":" Computationally, however, these"}]}]}
>```
>%%
>*%%PREFIX%%in the categories in each case.%%HIGHLIGHT%% ==For the purpose of enriching text rep-resentation, the categories tend to be “internal” categories that characterize a textobject (e.g., topical categories, sentiment categories). For the purpose of inferringproperties of associated entities with text data, the categories tend to be “external”categories that characterize an entity associated with the text object (e.g., author at-tribution or any other meaningful categories associated with text data, potentiallythrough indirect links).== %%POSTFIX%%Computationally, however, these*
>%%LINK%%[[#^9edgrq8pfgd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^9edgrq8pfgd


>%%
>```annotation-json
>{"created":"2024-09-11T09:41:40.472Z","updated":"2024-09-11T09:41:40.472Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":667481,"end":667868},{"type":"TextQuoteSelector","exact":"Such a rule-based manual approach would work well if: (1) the categories arevery clearly defined (usually means that the categories are relatively simple); (2) thecategories are easily distinguished based on surface features in text (e.g., particularwords only occur in a particular category of documents); and (3) sufficient domainknowledge is available to suggest many effective rules.","prefix":"cific problem of categorization.","suffix":"However, the manual approach has"}]}]}
>```
>%%
>*%%PREFIX%%cific problem of categorization.%%HIGHLIGHT%% ==Such a rule-based manual approach would work well if: (1) the categories arevery clearly defined (usually means that the categories are relatively simple); (2) thecategories are easily distinguished based on surface features in text (e.g., particularwords only occur in a particular category of documents); and (3) sufficient domainknowledge is available to suggest many effective rules.== %%POSTFIX%%However, the manual approach has*
>%%LINK%%[[#^kgecvz3smro|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^kgecvz3smro


>%%
>```annotation-json
>{"created":"2024-09-11T09:42:52.953Z","updated":"2024-09-11T09:42:52.953Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":668440,"end":668906},{"type":"TextQuoteSelector","exact":"These problems with the rule-based manual approach can mostly be addressedby using machine learning where humans would help the machine by labelingsome examples with the correct categories (i.e., creating training examples), andthe machine will learn from these examples to somewhat automatically constructrules for categorization, only that the rules are somewhat “soft” and weighted, andhow the rules should be combined is also learned based on the training data. ","prefix":"f application of differentrules.","suffix":"Notethat although in such a supe"}]}]}
>```
>%%
>*%%PREFIX%%f application of differentrules.%%HIGHLIGHT%% ==These problems with the rule-based manual approach can mostly be addressedby using machine learning where humans would help the machine by labelingsome examples with the correct categories (i.e., creating training examples), andthe machine will learn from these examples to somewhat automatically constructrules for categorization, only that the rules are somewhat “soft” and weighted, andhow the rules should be combined is also learned based on the training data.== %%POSTFIX%%Notethat although in such a supe*
>%%LINK%%[[#^5o9xe4qfz5h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5o9xe4qfz5h


>%%
>```annotation-json
>{"created":"2024-09-11T09:45:57.167Z","updated":"2024-09-11T09:45:57.167Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":669817,"end":670135},{"type":"TextQuoteSelector","exact":"In general, all these learning-based categorization methods rely on discrimina-tive features of text objects to distinguish categories, and they would combine mul-tiple features in a weighted manner where the weights are automatically learned(i.e., adjusted to minimize errors of categorization on the training data). ","prefix":" categorize any unseentext data.","suffix":"Differentmethods tend to vary in"}]}]}
>```
>%%
>*%%PREFIX%%categorize any unseentext data.%%HIGHLIGHT%% ==In general, all these learning-based categorization methods rely on discrimina-tive features of text objects to distinguish categories, and they would combine mul-tiple features in a weighted manner where the weights are automatically learned(i.e., adjusted to minimize errors of categorization on the training data).== %%POSTFIX%%Differentmethods tend to vary in*
>%%LINK%%[[#^and05qqpu1i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^and05qqpu1i


>%%
>```annotation-json
>{"created":"2024-09-11T11:10:21.625Z","updated":"2024-09-11T11:10:21.625Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":673139,"end":673386},{"type":"TextQuoteSelector","exact":"A classifier is a function f ( .) that takes a document vector as input and outputsa predicted label ˆy ∈ Y. Thus we could have f (x i) = sports. In this case, ˆy = sportsand the true y is also sports; the classifier was correct in its prediction.","prefix":" setup and yj could be politics.","suffix":"Notice how we can only evaluate "}]}]}
>```
>%%
>*%%PREFIX%%setup and yj could be politics.%%HIGHLIGHT%% ==A classifier is a function f ( .) that takes a document vector as input and outputsa predicted label ˆy ∈ Y. Thus we could have f (x i) = sports. In this case, ˆy = sportsand the true y is also sports; the classifier was correct in its prediction.== %%POSTFIX%%Notice how we can only evaluate*
>%%LINK%%[[#^3y4n09u7huv|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3y4n09u7huv


>%%
>```annotation-json
>{"created":"2024-09-11T12:14:55.882Z","updated":"2024-09-11T12:14:55.882Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":675578,"end":675820},{"type":"TextQuoteSelector","exact":"In Chapter 6 we emphasized the importance of the document representation inretrieval performance. In Chapter 2, we emphasized the importance of the featurerepresentation in general. The case is the same—if not greater—in text categoriza-tion.","prefix":"Features for Text Categorization","suffix":" Suppose we wish to determine wh"}]}]}
>```
>%%
>*%%PREFIX%%Features for Text Categorization%%HIGHLIGHT%% ==In Chapter 6 we emphasized the importance of the document representation inretrieval performance. In Chapter 2, we emphasized the importance of the featurerepresentation in general. The case is the same—if not greater—in text categoriza-tion.== %%POSTFIX%%Suppose we wish to determine wh*
>%%LINK%%[[#^u6eqbizse9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^u6eqbizse9


>%%
>```annotation-json
>{"created":"2024-09-11T12:16:46.143Z","updated":"2024-09-11T12:16:46.143Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":677773,"end":677929},{"type":"TextQuoteSelector","exact":"Clearly, a bigram words representation would most likely give better perfor-mance since we can capture not good and not bad as well as was good and was bad.","prefix":"ures for Text Categorization 305","suffix":"As a counterexample, using only "}]}]}
>```
>%%
>*%%PREFIX%%ures for Text Categorization 305%%HIGHLIGHT%% ==Clearly, a bigram words representation would most likely give better perfor-mance since we can capture not good and not bad as well as was good and was bad.== %%POSTFIX%%As a counterexample, using only*
>%%LINK%%[[#^reh3qfscg3h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^reh3qfscg3h


>%%
>```annotation-json
>{"created":"2024-09-11T12:17:49.733Z","updated":"2024-09-11T12:17:49.733Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":678311,"end":678471},{"type":"TextQuoteSelector","exact":"Due to this phe-nomenon, it is very common to combine multiple feature sets together. In thiscase, we can tokenize documents with both unigram and bigram words.","prefix":"ermining the sentence polarity. ","suffix":"A well-known strategy discussed "}]}]}
>```
>%%
>*%%PREFIX%%ermining the sentence polarity.%%HIGHLIGHT%% ==Due to this phe-nomenon, it is very common to combine multiple feature sets together. In thiscase, we can tokenize documents with both unigram and bigram words.== %%POSTFIX%%A well-known strategy discussed*
>%%LINK%%[[#^ez34hw9i27a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ez34hw9i27a


>%%
>```annotation-json
>{"created":"2024-09-11T12:18:27.034Z","updated":"2024-09-11T12:18:27.034Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":678471,"end":678773},{"type":"TextQuoteSelector","exact":"A well-known strategy discussed in Stamatatos [2009] shows that low-level lexicalfeatures combined with high-level syntactic features give the best performance in aclassifier. These two types of features are more orthogonal, thus capturing differentperspectives of the text to enrich the feature space.","prefix":"h both unigram and bigram words.","suffix":" Having many different types off"}]}]}
>```
>%%
>*%%PREFIX%%h both unigram and bigram words.%%HIGHLIGHT%% ==A well-known strategy discussed in Stamatatos [2009] shows that low-level lexicalfeatures combined with high-level syntactic features give the best performance in aclassifier. These two types of features are more orthogonal, thus capturing differentperspectives of the text to enrich the feature space.== %%POSTFIX%%Having many different types off*
>%%LINK%%[[#^nxgyu7gpav|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nxgyu7gpav


>%%
>```annotation-json
>{"created":"2024-09-11T12:20:19.207Z","updated":"2024-09-11T12:20:19.207Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":680136,"end":680316},{"type":"TextQuoteSelector","exact":"The authors foundthat these structural features combined with low-level lexical features (i.e., unigramwords) improved the classification accuracy over using only one feature type.","prefix":"ntactic category labels at all. ","suffix":"Another interesting feature gene"}]}]}
>```
>%%
>*%%PREFIX%%ntactic category labels at all.%%HIGHLIGHT%% ==The authors foundthat these structural features combined with low-level lexical features (i.e., unigramwords) improved the classification accuracy over using only one feature type.== %%POSTFIX%%Another interesting feature gene*
>%%LINK%%[[#^6lmqxtwexz9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^6lmqxtwexz9


>%%
>```annotation-json
>{"created":"2024-09-11T12:21:48.593Z","updated":"2024-09-11T12:21:48.593Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":680425,"end":680583},{"type":"TextQuoteSelector","exact":". The idea of SYNTACTICDIFF is to define threebasic (and therefore general) edit operations: insert a word, remove a word,and substitute one word for another.","prefix":" [2015] and called SYNTACTICDIFF","suffix":" These edits are used to transfo"}]}]}
>```
>%%
>*%%PREFIX%%[2015] and called SYNTACTICDIFF%%HIGHLIGHT%% ==. The idea of SYNTACTICDIFF is to define threebasic (and therefore general) edit operations: insert a word, remove a word,and substitute one word for another.== %%POSTFIX%%These edits are used to transfo*
>%%LINK%%[[#^2g4mxqs9w0m|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2g4mxqs9w0m


>%%
>```annotation-json
>{"created":"2024-09-11T12:23:37.090Z","updated":"2024-09-11T12:23:37.090Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":681576,"end":681972},{"type":"TextQuoteSelector","exact":"Again, it’s important to em-phasize that almost all machine learning algorithms are not affected by the type offeatures employed (in terms of operation; of course, the accuracy may be affected).Since internally the machine learning algorithms will simply refer to each featureas an ID, the algorithm may never even know if it’s operating on a parse tree, a word,bigram POS tags, or edit features.","prefix":"ive languages of essay writers. ","suffix":"The NLP pipeline discussed in Ch"}]}]}
>```
>%%
>*%%PREFIX%%ive languages of essay writers.%%HIGHLIGHT%% ==Again, it’s important to em-phasize that almost all machine learning algorithms are not affected by the type offeatures employed (in terms of operation; of course, the accuracy may be affected).Since internally the machine learning algorithms will simply refer to each featureas an ID, the algorithm may never even know if it’s operating on a parse tree, a word,bigram POS tags, or edit features.== %%POSTFIX%%The NLP pipeline discussed in Ch*
>%%LINK%%[[#^f0vy3hwmdcl|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f0vy3hwmdcl


>%%
>```annotation-json
>{"created":"2024-09-11T12:39:10.924Z","updated":"2024-09-11T12:39:10.924Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":697740,"end":698553},{"type":"TextQuoteSelector","exact":"Another evaluation paradigm is n-fold cross validation. This splits the corpusinto n partitions. In n rounds, one partition is selected as the testing set and theremaining n − 1 are used for training. The final accuracy, F1 score, or any otherevaluation metric is then averaged over the n folds. The variance in scores betweenthe folds can be a hint at the overfitting potential of your algorithm. If the varianceis high, it means that the accuracies are not very similar between folds. Havingone fold with a very high accuracy suggests that your learning algorithm may haveoverfit during that training stage; when using that trained algorithm on a separatecorpus, it’s likely that the accuracy would be very low since it modeled noise or otheruninformative features from that particular split (i.e., it overfit).","prefix":"f it is not prone tooverfitting.","suffix":"Another important concept is bas"}]}]}
>```
>%%
>*%%PREFIX%%f it is not prone tooverfitting.%%HIGHLIGHT%% ==Another evaluation paradigm is n-fold cross validation. This splits the corpusinto n partitions. In n rounds, one partition is selected as the testing set and theremaining n − 1 are used for training. The final accuracy, F1 score, or any otherevaluation metric is then averaged over the n folds. The variance in scores betweenthe folds can be a hint at the overfitting potential of your algorithm. If the varianceis high, it means that the accuracies are not very similar between folds. Havingone fold with a very high accuracy suggests that your learning algorithm may haveoverfit during that training stage; when using that trained algorithm on a separatecorpus, it’s likely that the accuracy would be very low since it modeled noise or otheruninformative features from that particular split (i.e., it overfit).== %%POSTFIX%%Another important concept is bas*
>%%LINK%%[[#^teota1ev5rh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^teota1ev5rh


>%%
>```annotation-json
>{"created":"2024-09-15T19:57:16.863Z","updated":"2024-09-15T19:57:16.863Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":729944,"end":730104},{"type":"TextQuoteSelector","exact":"For example, we can talk about the topicof a sentence, the topic of an article, the topic of a paragraph, or the topic of allthe research articles in a library.","prefix":"n have different granularities. ","suffix":" Different granularities of topi"}]}]}
>```
>%%
>*%%PREFIX%%n have different granularities.%%HIGHLIGHT%% ==For example, we can talk about the topicof a sentence, the topic of an article, the topic of a paragraph, or the topic of allthe research articles in a library.== %%POSTFIX%%Different granularities of topi*
>%%LINK%%[[#^afc0v6nyar5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^afc0v6nyar5


>%%
>```annotation-json
>{"created":"2024-09-15T19:58:25.889Z","updated":"2024-09-15T19:58:25.889Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":731194,"end":731473},{"type":"TextQuoteSelector","exact":"We can view a topic as describing some knowledge about the world as shownin Figure 17.1. From text data, we want to discover a number of topics which canprovide a description about the world. That is, a topic tells us something about theworld (e.g., about a product or a person).","prefix":"is a main topic of this chapter.","suffix":"Besides text data, we often also"}]}]}
>```
>%%
>*%%PREFIX%%is a main topic of this chapter.%%HIGHLIGHT%% ==We can view a topic as describing some knowledge about the world as shownin Figure 17.1. From text data, we want to discover a number of topics which canprovide a description about the world. That is, a topic tells us something about theworld (e.g., about a product or a person).== %%POSTFIX%%Besides text data, we often also*
>%%LINK%%[[#^ztiqs57vuci|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ztiqs57vuci


>%%
>```annotation-json
>{"created":"2024-09-15T19:59:10.318Z","updated":"2024-09-15T19:59:10.318Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":734748,"end":734909},{"type":"TextQuoteSelector","exact":"All such metadata (or context variables) canbe associated with the topics that we discover, and we can then use these contextvariables to analyze topic patterns.","prefix":"xt, or the sources of the text. ","suffix":"For example, looking at topics o"}]}]}
>```
>%%
>*%%PREFIX%%xt, or the sources of the text.%%HIGHLIGHT%% ==All such metadata (or context variables) canbe associated with the topics that we discover, and we can then use these contextvariables to analyze topic patterns.== %%POSTFIX%%For example, looking at topics o*
>%%LINK%%[[#^knxdul9nmd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^knxdul9nmd


>%%
>```annotation-json
>{"created":"2024-09-15T20:01:16.071Z","updated":"2024-09-15T20:01:16.071Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":735173,"end":735436},{"type":"TextQuoteSelector","exact":"Let’s look at the tasks of topic mining and analysis. As shown in Figure 17.2,topic analysis first involves discovering a number of topics. In this case, there are ktopics. We also would like to know which topics are covered in which documents,and to what extent.","prefix":" opinions indifferent locations.","suffix":" For example, in Doc 1, the visu"}]}]}
>```
>%%
>*%%PREFIX%%opinions indifferent locations.%%HIGHLIGHT%% ==Let’s look at the tasks of topic mining and analysis. As shown in Figure 17.2,topic analysis first involves discovering a number of topics. In this case, there are ktopics. We also would like to know which topics are covered in which documents,and to what extent.== %%POSTFIX%%For example, in Doc 1, the visu*
>%%LINK%%[[#^41zfvlelysx|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^41zfvlelysx


>%%
>```annotation-json
>{"created":"2024-09-15T20:01:27.521Z","updated":"2024-09-15T20:01:27.521Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":735698,"end":735905},{"type":"TextQuoteSelector","exact":"Thus, there are generally two different tasks or sub-tasks: the first is to discover the k topics from a collection of text; the second taskis to figure out which documents cover which topics to what extent.","prefix":"ocovers Topic k to some extent. ","suffix":"More formally, we can define the"}]}]}
>```
>%%
>*%%PREFIX%%ocovers Topic k to some extent.%%HIGHLIGHT%% ==Thus, there are generally two different tasks or sub-tasks: the first is to discover the k topics from a collection of text; the second taskis to figure out which documents cover which topics to what extent.== %%POSTFIX%%More formally, we can define the*
>%%LINK%%[[#^4yx89t1ltjd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4yx89t1ltjd


>%%
>```annotation-json
>{"created":"2024-09-15T20:03:35.173Z","updated":"2024-09-15T20:03:35.173Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":739848,"end":740111},{"type":"TextQuoteSelector","exact":"For each document,we have a set of such π values to indicate to what extent the document covers eachtopic. We assume that these probabilities sum to one, which means that we assumea document won’t be able to cover other topics outside of the topics we discovered.","prefix":"document di covering topic θj . ","suffix":"Now, the next question is, how d"}]}]}
>```
>%%
>*%%PREFIX%%document di covering topic θj .%%HIGHLIGHT%% ==For each document,we have a set of such π values to indicate to what extent the document covers eachtopic. We assume that these probabilities sum to one, which means that we assumea document won’t be able to cover other topics outside of the topics we discovered.== %%POSTFIX%%Now, the next question is, how d*
>%%LINK%%[[#^5vfk9v3swc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5vfk9v3swc


>%%
>```annotation-json
>{"created":"2024-09-15T20:06:32.853Z","updated":"2024-09-15T20:06:32.853Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":866648,"end":866859},{"type":"TextQuoteSelector","exact":"Topic analysis evaluation has similar difficulties to information retrieval evalua-tion. In both cases, there is usually not one true answer, and evaluation metricsheavily depend on the human issuing judgements.","prefix":"r.17.6 Evaluating Topic Analysis","suffix":" What defines a topic? We ad-dre"}]}]}
>```
>%%
>*%%PREFIX%%r.17.6 Evaluating Topic Analysis%%HIGHLIGHT%% ==Topic analysis evaluation has similar difficulties to information retrieval evalua-tion. In both cases, there is usually not one true answer, and evaluation metricsheavily depend on the human issuing judgements.== %%POSTFIX%%What defines a topic? We ad-dre*
>%%LINK%%[[#^j9a3hmjtpoa|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^j9a3hmjtpoa


>%%
>```annotation-json
>{"created":"2024-09-15T20:11:11.764Z","updated":"2024-09-15T20:11:11.764Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":867064,"end":867521},{"type":"TextQuoteSelector","exact":"Log-likelihood and model perplexity are two common evaluation measures usedby language models, and they can be applied for topic analysis in the same way. Bothare predictive measures, meaning that held-out data is presented to the model andthe model is applied to this new information, calculating its likelihood. If the modelgeneralizes well to this new data (by assigning it a high likelihood or low perplexity),then the model is assumed to be sufficient.","prefix":"s the eventual eval-uation task.","suffix":"In Chapter 13, we mentioned Chan"}]}]}
>```
>%%
>*%%PREFIX%%s the eventual eval-uation task.%%HIGHLIGHT%% ==Log-likelihood and model perplexity are two common evaluation measures usedby language models, and they can be applied for topic analysis in the same way. Bothare predictive measures, meaning that held-out data is presented to the model andthe model is applied to this new information, calculating its likelihood. If the modelgeneralizes well to this new data (by assigning it a high likelihood or low perplexity),then the model is assumed to be sufficient.== %%POSTFIX%%In Chapter 13, we mentioned Chan*
>%%LINK%%[[#^eapwj7sb2v|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^eapwj7sb2v


>%%
>```annotation-json
>{"created":"2024-09-15T20:13:11.178Z","updated":"2024-09-15T20:13:11.178Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":868902,"end":869140},{"type":"TextQuoteSelector","exact":"“Traditional metrics” refers to log-likelihood of held-out data in the case of gen-erative models. This misalignment of results is certainly a pressing issue, thoughmostrecentresearchstillreliesonthetraditionalmeasurestoevaluatenewmodels.","prefix":"th themeasures of topic quality.","suffix":"Downstream task improvement is p"}]}]}
>```
>%%
>*%%PREFIX%%th themeasures of topic quality.%%HIGHLIGHT%% ==“Traditional metrics” refers to log-likelihood of held-out data in the case of gen-erative models. This misalignment of results is certainly a pressing issue, thoughmostrecentresearchstillreliesonthetraditionalmeasurestoevaluatenewmodels.== %%POSTFIX%%Downstream task improvement is p*
>%%LINK%%[[#^pqwdjn4ra1|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pqwdjn4ra1


>%%
>```annotation-json
>{"created":"2024-09-15T20:13:59.295Z","updated":"2024-09-15T20:13:59.295Z","document":{"title":"Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining","link":[{"href":"urn:x-pdf:a97cd36006c8be145ebab77b4be0f3a0"},{"href":"vault:/zhai2016.pdf"}],"documentFingerprint":"a97cd36006c8be145ebab77b4be0f3a0"},"uri":"vault:/zhai2016.pdf","target":[{"source":"vault:/zhai2016.pdf","selector":[{"type":"TextPositionSelector","start":869551,"end":869770},{"type":"TextQuoteSelector","exact":"In such a case, log-likelihood of held-out data and even topic coherency is not aconcern if the classification accuracy improves—although model interpretabilitymay be compromised if topics are not human-distinguishable.","prefix":"the metric we’d wish to improve.","suffix":"17.7 Summary of Topic ModelsIn s"}]}]}
>```
>%%
>*%%PREFIX%%the metric we’d wish to improve.%%HIGHLIGHT%% ==In such a case, log-likelihood of held-out data and even topic coherency is not aconcern if the classification accuracy improves—although model interpretabilitymay be compromised if topics are not human-distinguishable.== %%POSTFIX%%17.7 Summary of Topic ModelsIn s*
>%%LINK%%[[#^onwg62yohlg|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^onwg62yohlg
